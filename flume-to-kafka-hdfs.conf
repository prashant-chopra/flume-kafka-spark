
# flume-to-kafka-hdfs.conf: a multiplex agent to save one copy of data in HDFS and
# other copy streamed to Kafka so that data can be processed by
# streaming technologies such as Spark Streaming

# Name the components on this agent
kafkapc.sources = logsource
kafkapc.sinks = kafkasink hdfssink
kafkapc.channels = kafkachannel hdfschannel

# Describe/configure the source
kafkapc.sources.logsource.type = exec
kafkapc.sources.logsource.command = tail -F /opt/gen_logs/logs/access.log

# Describe the kafka sink
kafkapc.sinks.kafkasink.type = org.apache.flume.sink.kafka.KafkaSink
kafkapc.sinks.kafkasink.brokerList = nn02.mycluster.com:6667
kafkapc.sinks.kafkasink.topic = wlkafkapc

#Desceribe the hdfs sink
kafkapc.sinks.hdfssink.type = hdfs
kafkapc.sinks.hdfssink.hdfs.path = hdfs://nn01.mycluster.com:8020/user/prashantchopra/labs/kafkapc
kafkapc.sinks.hdfssink.hdfs.fileType = DataStream
kafkapc.sinks.hdfssink.hdfs.rollInterval = 120
kafkapc.sinks.hdfssink.hdfs.rollSize = 10485760
kafkapc.sinks.hdfssink.hdfs.rollCount = 30
kafkapc.sinks.hdfssink.hdfs.filePrefix = retail
kafkapc.sinks.hdfssink.hdfs.fileSuffix = .txt
kafkapc.sinks.hdfssink.hdfs.inUseSuffix = .tmp
kafkapc.sinks.hdfssink.hdfs.useLocalTimeStamp = true

# Use a channel which buffers events in memory
kafkapc.channels.kafkachannel.type = memory
kafkapc.channels.kafkachannel.capacity = 1000
kafkapc.channels.kafkachannel.transactionCapacity = 100

#Use a channel which buffers events in file for HDFS sink
kafkapc.channels.hdfschannel.type = file
kafkapc.channels.hdfschannel.capacity = 1000
kafkapc.channels.hdfschannel.transactionCapacity = 100
kafkapc.channels.hdfschannel.checkpointInterval = 300

# Bind the source and sink to the channel
kafkapc.sources.logsource.channels = hdfschannel kafkachannel
kafkapc.sinks.kafkasink.channel = kafkachannel
kafkapc.sinks.hdfssink.channel = hdfschannel
